<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Tanmay Khandelwal | Publications</title>
  <meta name="description" content="A beautiful Jekyll theme for academics">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tanmay</span> Khandelwal</a>
     	<div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center" style="line-height: 1em;">
            <a href="mailto:tk3309@nyu.edu"><i class="fa fa-envelope-square gm-icon"></i></a>
            <a href="https://scholar.google.com/citations?user=UhVMRBQAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar-square gs-icon"></i></a>
            <a href="https://github.com/tanmayy24" target="_blank" title="GitHub"><i class="fab fa-github-square gh-icon"></i></a>
          </span>
        </div>
 
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
              <li class="nav-item ">
                  <a class="nav-link" href="/assets/pdf/vitae.pdf">
                    Curriculum Vitae
                  </a>
              </li>
           
		          <!-- 
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>
            	-->

              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
<!--             
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    Teaching
                    
                  </a>
              </li> -->
<!-- 
              <li class="nav-item ">
                  <a class="nav-link" href="/service/">
                    Service
                                        
                  </a>
              </li> -->
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6>


<p><br /></p>


<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2023</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">


<li><div class="row m-0 mt-3 p-0">
     <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://cdc2023.ieeecss.org/" target="_blank">
          CDC
        </a>
     </div>

      <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

        <div id="lourenco2023CDC" class="col p-0">
          <h5 class="title mb-0">Diagnosing and Augmenting Feature Representations in Correctional Inverse Reinforcement Learning</h5>
          <div class="author">

              <nobr><a href="https://www.kth.se/profile/ineslo" target="_blank">Inês Lourenço</a>,</nobr>

              <nobr><em>Tanmay Khandelwal</em>,</nobr>

             <nobr><a href="https://www.kth.se/profile/crro" target="_blank">Cristian R. Rojas</a>,</nobr>

                                        and

              <nobr><a href="https://www.kth.se/profile/bo" target="_blank">Bo Wahlberg</a>.</nobr>

          </div>

          <div>
            <p class="periodical font-italic">
              Conference on Decision and Control (CDC), 2023
            </p>
          </div>

          <div class="col p-0">

              <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#lourenco2023CDC-abstract" role="button" aria-expanded="false" aria-controls="lourenco2023CDC-abstract">Abstract</a>

              <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/pdf/2304.05238.pdf" target="_blank">PDF</a>


          </div>

          <div class="col mt-2 p-0">
            <div id="lourenco2023CDC-abstract" class="collapse">
                <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    Robots have been increasingly better at doing tasks for humans by learning from their feedback, but still often suffer from model misalignment due to missing or incorrectly learned features.
                    When the features the robot needs to learn to perform its task are missing or do not generalize well to new settings, the robot will not be able to learn the task the human wants and, even worse, may learn a completely different and undesired behavior.
                    Prior work shows how the robot can detect when its representation is missing some feature and can, thus, ask the human to be taught about the new feature; however, these works do not differentiate between features that are completely missing and those that exist but do not generalize to new environments.
                    In the latter case, the robot would detect misalignment and simply learn a new feature, leading to an arbitrarily growing feature representation that can, in turn, lead to spurious correlations and incorrect learning down the line.
                    In this work, we propose separating the two sources of misalignment: we propose a framework for determining whether a feature the robot needs is incorrectly learned and does not generalize to new environment setups vs. is entirely missing from the robot's representation.
                    Once we detect the source of error, we show how the human can initiate the realignment process for the model: if the feature is missing, we follow prior work for learning new features; however, if the feature exists but does not generalize, we use data augmentation to expand its training and, thus, complete the correction.
                    We demonstrate the proposed approach in experiments with a simulated 7DoF robot manipulator and physical human corrections.
                </div>
            </div>
          </div>

        </div>
      </div>
    </div>
</li>

  <li><div class="row m-0 mt-3 p-0">
     <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://icml.cc/" target="_blank">
          ICML
        </a>
     </div>

      <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

        <div id="peng2023ICML" class="col p-0">
          <h5 class="title mb-0">Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation</h5>
          <div class="author">

              <nobr><a href="https://andipeng.com/" target="_blank">Andi Peng</a>,</nobr>

              <nobr><a href="https://avivne.github.io/" target="_blank">Aviv Netanyahu</a>,</nobr>

              <nobr><a href="https://markkho.github.io/" target="_blank">Mark K. Ho</a>,</nobr>

              <nobr><a href="https://www.tshu.io/" target="_blank">Tianmin Shu</a>,</nobr>

              <nobr><em>Tanmay Khandelwal</em>,</nobr>

             <nobr><a href="https://interactive.mit.edu/about/people/julie" target="_blank">Julie Shah</a>,</nobr>

                                        and

              <nobr><a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>.</nobr>

          </div>

          <div>
            <p class="periodical font-italic">
              International Conference on Machine Learning (ICML), 2023
            </p>
          </div>

          <div class="col p-0">

              <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#peng2023ICML-abstract" role="button" aria-expanded="false" aria-controls="peng2023ICML-abstract">Abstract</a>

              <a class="badge grey waves-effect font-weight-light mr-1" href="https://openreview.net/pdf?id=rFLtREMkTR" target="_blank">PDF</a>

          </div>

          <div class="col mt-2 p-0">
            <div id="peng2023ICML-abstract" class="collapse">
                <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                  Policies often fail at test-time due to distribution shifts---changes in the state and reward that occur when an end user deploys the policy in environments different from those seen in training.
                  Data augmentation can help models be more robust to such shifts by varying specific concepts in the state, e.g. object color, that are task-irrelevant and should not impact desired actions.
                  However, designers training the agent don't often know which concepts are irrelevant a priori.
                  We propose a human-in-the-loop framework to leverage feedback from the end user to quickly identify and augment task-irrelevant visual state concepts.
                  Our framework generates counterfactual demonstrations that allow users to quickly isolate shifted state concepts and identify if they should not impact the desired task, and can therefore be augmented using existing actions.
                  We present experiments validating our full pipeline on discrete and continuous control tasks with real human users.
                  Our method better enables users to (1) understand agent failure, (2) improve sample efficiency of demonstrations required for finetuning, and (3) adapt the agent to their desired reward.
                </div>
            </div>
          </div>

        </div>
      </div>
    </div>
</li>


		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://humanrobotinteraction.org/2023/" target="_blank">
          HRI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

    <div id="bobu2023HRI" class="col p-0">
      <h5 class="title mb-0">SIRL: Similarity-based Implicit Representation Learning</h5>
      <div class="author">

									<nobr><em>Tanmay Khandelwal*</em>,</nobr>

                  <nobr><a href="https://www.linkedin.com/in/yiliu77/" target="_blank">Yi Liu<nobr><em>*</em></nobr></a>,</nobr>

                  <nobr><a href="https://rohinshah.com/" target="_blank">Rohin Shah</a>,</nobr>

                  <nobr><a href="https://www.cs.utah.edu/~dsbrown/" target="_blank">Daniel S. Brown</a>,</nobr>

									and

                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>

      </div>

      <div>
        <p class="periodical font-italic">
          ACM/IEEE International Conference on Human Robot Interaction (HRI), 2023
        </p>
      </div>

      <div class="col p-0">

          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2023HRI-abstract" role="button" aria-expanded="false" aria-controls="bobu2023HRI-abstract">Abstract</a>

          <a class="badge grey waves-effect font-weight-light mr-1" href="https://dl.acm.org/doi/abs/10.1145/3568162.3576989" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/T5Q4-Vxqbo4" target="_blank">Talk</a>

      </div>

      <div class="col mt-2 p-0">
        <div id="bobu2023HRI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
              When robots learn reward functions using high capacity models that take raw state directly as input, they need to both learn a representation for what matters in the task -- the task "features" -- as well as how to combine these features into a single objective.
              If they try to do both at once from input designed to teach the full reward function, it is easy to end up with a representation that contains spurious correlations in the data, which fails to generalize to new settings.
              Instead, our ultimate goal is to enable robots to identify and isolate the causal features that people actually care about and use when they represent states and behavior.
              Our idea is that we can tune into this representation by asking users what behaviors they consider similar: behaviors will be similar if the features that matter are similar, even if low-level behavior is different; conversely, behaviors will be different if even one of the features that matter differs.
              This, in turn, is what enables the robot to disambiguate between what needs to go into the representation versus what is spurious, as well as what aspects of behavior can be compressed together versus not. The notion of learning representations based on similarity has a nice parallel in contrastive learning, a self-supervised representation learning technique that maps visually similar data points to similar embeddings, where similarity is defined by a designer through data augmentation heuristics. By contrast, in order to learn the representations that people use, so we can learn their preferences and objectives, we use their definition of similarity. In simulation as well as in a user study, we show that learning through such similarity queries leads to representations that, while far from perfect, are indeed more generalizable than self-supervised and task-input alternatives.          </div>
        </div>
      </div>

    </div>
  </div>
</div>
</li>

      </ol>
    </div>
  </div>



<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">


<li><div class="row m-0 mt-3 p-0">
	<div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://neurips-hill.github.io/" target="_blank">
          HILL
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

    <div id="zhang2022NeurIPSw" class="col p-0">
      <h5 class="title mb-0">Time-Efficient Reward Learning via Visually Assisted Cluster Ranking</h5>
      <div class="author">

                  <nobr><a href="https://www.linkedin.com/in/davidzhang501/" target="_blank">David Zhang</a>,</nobr>

                  <nobr><a href="https://micahcarroll.github.io/" target="_blank">Micah Carroll</a>,</nobr>

							<nobr><em>Tanmay Khandelwal</em>,</nobr>

									and

                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>

      </div>

      <div>
        <p class="periodical font-italic">
        Workshop on Workshop on Human in the Loop Learning, NeurIPS 2022
        </p>
      </div>

      <div class="col p-0">

          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zhang2022NeurIPSw-abstract" role="button" aria-expanded="false" aria-controls="zhang2022NeurIPSw-abstract">Abstract</a>

          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2212.00169" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://drive.google.com/drive/u/0/folders/1hxsVqD940LMYfGusKe5SB3AXtQJNcH9l" target="_blank">Video</a>
      </div>

      <div class="col mt-2 p-0">
        <div id="zhang2022NeurIPSw-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	    One of the most successful paradigms for reward learning uses human feedback in the form of comparisons.
              Although these methods hold promise, human comparison labeling is expensive and time consuming, constituting a major bottleneck to their broader applicability.
              Our insight is that we can greatly improve how effectively human time is used in these approaches by batching comparisons together, rather than having the human label each comparison individually.
              To do so, we leverage data dimensionality-reduction and visualization techniques to provide the human with a interactive GUI displaying the state space, in which the user can label subportions of the state space.
              Across some simple Mujoco tasks, we show that this high-level approach holds promise and is able to greatly increase the performance of the resulting agents, provided the same amount of human labeling time.
	</div>
        </div>
      </div>

    </div>
  </div>
</div>
</li>


<li><div class="row m-0 mt-3 p-0">
	<div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.cobot-wotf-workshop.com/accepted-papers" target="_blank">
          CoR-WotF
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2022ICRAw" class="col p-0">
      <h5 class="title mb-0">Aligning Robot Representations with Humans</h5>
      <div class="author">
                
							<nobr><em>Tanmay Khandelwal</em></nobr>              
                
									and
                
                  <nobr><a href="https://andipeng.com/" target="_blank">Andi Peng</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        Workshop on Collaborative Robots and the Work of the Future, ICRA 2022
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2022ICRAw-abstract" role="button" aria-expanded="false" aria-controls="bobu2022ICRAw-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/jobvision/bobu2022ICRAw.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://drive.google.com/file/d/1KydXhKeX9bBOUbnkPkQZjTsao0XqcrEw/view" target="_blank">Video</a>

          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/jobvision/bobu2022ICRAw_poster.pdf" target="_blank">Poster</a>
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2022ICRAw-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	As robots are increasingly deployed in real-world scenarios, a key question is how to best transfer knowledge learned in one environment to another, where shifting constraints and human preferences render adaptation challenging. A central challenge remains that often, it is difficult (perhaps even impossible) to capture the full complexity of the deployment environment, and therefore the desired tasks, at training time. Consequently, the representation, or abstraction, of the tasks the human hopes for the robot to perform in one environment may be misaligned with the representation of the tasks that the robot has learned in another. We postulate that because humans will be the ultimate evaluator of system success in the world, they are best suited to communicating the aspects of the tasks that matter to the robot. Our key insight is that effective learning from human input requires first explicitly learning good intermediate representations and then using those representations for solving downstream tasks. We highlight three areas where we can use this approach to build interactive systems and offer future directions of work to better create advanced collaborative robots.
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>



<li><div class="row m-0 mt-3 p-0">
	<div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://iros2022.org/" target="_blank">
          IROS
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="sripathy2022IROS" class="col p-0">
      <h5 class="title mb-0">Teaching Robots to Span the Space of Functional Expressive Motion</h5>
      <div class="author">
                
                  <nobr><a href="https://www.linkedin.com/in/arjunsripathy" target="_blank">Arjun Sripathy</a>,</nobr>
		
							<nobr><em>Tanmay Khandelwal</em>,</nobr>              
                
                  <nobr><a href="https://www.linkedin.com/in/zhongyu-li-b2b8aa160/" target="_blank">Zhongyu Li</a>,</nobr>

                  <nobr><a href="https://me.berkeley.edu/people/koushil-sreenath/" target="_blank">Koushil Sreenath</a>,</nobr>

                  <nobr><a href="https://www.cs.utah.edu/~dsbrown/" target="_blank">Daniel S. Brown</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#sripathy2022IROS-abstract" role="button" aria-expanded="false" aria-controls="sripathy2022IROS-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/cassie/sripathy2022IROS.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arjunsripathy.github.io/robot_emotive_space/" target="_blank">Project Website</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/arjunsripathy/robot_emotive_space" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=k8pMq4WcEgQ&t=3s&ab_channel=ArjunSripathy" target="_blank">Video</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="sripathy2022IROS-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
        Our goal is to enable robots to perform functional tasks in emotive ways, be it in response to their users' emotional states, or expressive of their confidence levels. Prior work has proposed learning independent cost functions from user feedback for each target emotion, so that the robot may optimize it alongside task and environment specific objectives for any situation it encounters. However, this approach is inefficient when modeling multiple emotions and unable to generalize to new ones. In this work, we leverage the fact that emotions are not independent of each other: they are related through a latent space of Valence-Arousal-Dominance (VAD). Our key idea is to learn a model for how trajectories map onto VAD with user labels. Considering the distance between a trajectory's mapping and a target VAD allows this single model to represent cost functions for all emotions. As a result 1) all user feedback can contribute to learning about every emotion; 2) the robot can generate trajectories for any emotion in the space instead of only a few predefined ones; and 3) the robot can respond emotively to user-generated natural language by mapping it to a target VAD. We introduce a method that interactively learns to map trajectories to this latent space and test it in simulation and in a user study. In experiments, we use a simple vacuum robot as well as the Cassie biped.  
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>



		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.ieee-ras.org/publications/ra-l" target="_blank">
          RA-L
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2022RAL" class="col p-0">
      <h5 class="title mb-0">Learning Perceptual Concepts by Bootstrapping from Human Queries</h5>
      <div class="author">
                
		
							<nobr><em>Tanmay Khandelwal</em>,</nobr>              
                
                  <nobr><a href="https://cpaxton.github.io/about/" target="_blank">Chris Paxton</a>,</nobr>

                  <nobr><a href="https://research.nvidia.com/person/wei-yang" target="_blank">Wei Yang</a>,</nobr>

                  <nobr><a href="https://balakumar-s.github.io/" target="_blank">Balakumar Sundaralingam</a>,</nobr>

                  <nobr><a href="https://research.nvidia.com/person/yuwei-chao" target="_blank">Yu-Wei Chao</a>,</nobr>

                  <nobr><a href="https://homes.cs.washington.edu/~mcakmak/" target="_blank">Maya Cakmak</a>,</nobr>

									and
                
                  <nobr><a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        IEEE Robotics and Automation Letters (RA-L), 2022
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2022RAL-abstract" role="button" aria-expanded="false" aria-controls="bobu2022RAL-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/NVIDIA/bobu2022RAL.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/nvidia.com/active-concept-learning" target="_blank">Project Website</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/tanmayy24/concept_learning" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=LszKCwmOle8&feature=youtu.be&ab_channel=AndreeaBobu" target="_blank">Video</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/NVIDIA/bobu2022RAL_poster.pdf" target="_blank">Poster</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://patents.google.com/patent/US20230145208A1/en" target="_blank">Patent</a>
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2022RAL-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
Most robot tasks can be thought of as relating one or more objects, and learning new tasks by necessity involves teaching the robot new concepts relating objects to one another. However, learning new concepts that operate on high-dimensional data – like that coming from a robot’s sensors – is impractical, because it requires an unrealistic amount of labeled human input. In this work, we observe that by using a simulator at training time we can get access to significant privileged information – things like object poses and bounding boxes – that allows for learning a low-dimensional variant of the concept with much less human input. The robot can then use this low-dimensional concept to automatically label large amounts of high-dimensional data in the simulator. This enables learning perceptual concepts that work with real sensor input where no privileged information is available. We evaluate our Perceptual Concept Bootstrapping (PCB) approach by learning spatial concepts that describe object state or multi-object relationships. We show that our approach improves sample complexity when compared to learning concepts directly in the high-dimensional space. We also demonstrate the utility of the learned concepts in motion planning tasks on a 7-DoF Franka Panda robot.
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>



	<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://journals.sagepub.com/home/ijr" target="_blank">
          IJRR
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2022IJRR" class="col p-0">
      <h5 class="title mb-0">Inducing Structure in Reward Learning by Learning Features</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal</em>,</nobr>
              
                  <nobr><a href="https://www.linkedin.com/in/marius-wiggert/" target="_blank">Marius Wiggert<nobr></nobr></a>,</nobr>
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~tomlin/" target="_blank">Claire J. Tomlin</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          The International Journal of Robotics Research (IJRR)
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2022IJRR-abstract" role="button" aria-expanded="false" aria-controls="bobu2022IJRR-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/FERL/bobu2022IJRR.pdf" target="_blank">PDF</a>
          
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2022IJRR-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	Reward learning enables robots to learn adaptable behaviors from human input. Traditional methods model the reward as a linear function of hand-crafted features, but that requires specifying all the relevant features a priori, which is impossible for real-world tasks. To get around this issue, recent deep Inverse Reinforcement Learning (IRL) methods learn rewards directly from the raw state but this is challenging because the robot has to implicitly learn the features that are important and how to combine them, simultaneously. Instead, we propose a divide and conquer approach: focus human input specifically on learning the features separately, and only then learn how to combine them into a reward. We introduce a novel type of human input for teaching features and an algorithm that utilizes it to learn complex features from the raw state space. The robot can then learn how to combine them into a reward using demonstrations, corrections, or other reward learning frameworks. We demonstrate our method in settings where all features have to be learned from scratch, as well as where some of the features are known. By first focusing human input specifically on the feature(s), our method decreases sample complexity and improves generalization of the learned reward over a deepIRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
			
</li></ol>

			
			
			</ol>
    </div>
  </div>





<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2021</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
	

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://ras.papercept.net/conferences/conferences/ICRA21/program/ICRA21_ProgramAtAGlanceWeb.html" target="_blank">
          ICRA
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="zurek2021ICRA" class="col p-0">
      <h5 class="title mb-0">Situational Confidence Assistance for Lifelong Shared Autonomy</h5>
      <div class="author">
                
                  <nobr>Matthew Zurek<nobr><em>*</em></nobr></a>,</nobr>
		
							<nobr><em>Tanmay Khandelwal*</em>,</nobr>              
                
                  <nobr><a href="https://www.cs.utah.edu/~dsbrown/" target="_blank">Daniel S. Brown</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        International Conference on Robot Learning (ICRA), 2021
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zurek2021ICRA-abstract" role="button" aria-expanded="false" aria-controls="zurek2021ICRA-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CASA/zurek2021ICRA.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/A4k3B3uewBs?t=8337" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="zurek2021ICRA-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
	Shared autonomy enables robots to infer user intent and assist in accomplishing it. But when the user wants to do a new task that the robot does not know about, shared autonomy will hinder their performance by attempting to assist them with something that is not their intent. Our key idea is that the robot can detect when its repertoire of intents is insufficient to explain the user's input, and give them back control. This then enables the robot to observe unhindered task execution, learn the new intent behind it, and add it to this repertoire. We demonstrate with both a case study and a user study that our proposed method maintains good performance when the human's intent is in the robot's repertoire, outperforms prior shared autonomy approaches when it isn't, and successfully learns new skills, enabling efficient lifelong learning for confidence-based shared autonomy.          
</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://ras.papercept.net/conferences/conferences/ICRA21/program/ICRA21_ProgramAtAGlanceWeb.html" target="_blank">
          ICRA
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="sripathy2021ICRA" class="col p-0">
      <h5 class="title mb-0">Dynamically Switching Human Prediction Models for Efficient Planning</h5>
      <div class="author">
                
                  <nobr><a href="https://www.linkedin.com/in/arjunsripathy" target="_blank">Arjun Sripathy<nobr><em>*</em></nobr></a>,</nobr>
		
							<nobr><em>Tanmay Khandelwal*</em>,</nobr>              
                
                  <nobr><a href="https://www.cs.utah.edu/~dsbrown/" target="_blank">Daniel S. Brown</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
        International Conference on Robot Learning (ICRA), 2021
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#sripathy2021ICRA-abstract" role="button" aria-expanded="false" aria-controls="sripathy2021ICRA-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/modelswitching/sripathy2021ICRA.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arjunsripathy.github.io/model_switching/" target="_blank">Project Website</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/arjunsripathy/model_switching" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=g1x-gQYYSag&t=14s&ab_channel=ArjunSripathy" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="sripathy2021ICRA-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
		As environments involving both robots and humans become increasingly common, so does the need to account for people during planning. To plan effectively, robots must be able to respond to and sometimes influence what humans do. This requires a human model which predicts future human actions. A simple model may assume the human will continue what they did previously; a more complex one might predict that the human will act optimally, disregarding the robot; whereas an even more complex one might capture the robot's ability to influence the human. These models make different trade-offs between computational time and performance of the resulting robot plan. Using only one model of the human either wastes computational resources or is unable to handle critical situations. In this work, we give the robot access to a suite of human models and enable it to assess the performance-computation trade-off online. By estimating how an alternate model could improve human prediction and how that may translate to performance gain, the robot can dynamically switch human models whenever the additional computation is justified. Our experiments in a driving simulator showcase how the robot can achieve performance comparable to always using the best human model, but with greatly reduced computation.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>





	<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://humanrobotinteraction.org/2021/" target="_blank">
          HRI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2021HRI" class="col p-0">
      <h5 class="title mb-0">Feature Expansive Reward Learning: Rethinking Human Input</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal*</em>,</nobr>
              
                  <nobr><a href="https://www.linkedin.com/in/marius-wiggert/" target="_blank">Marius Wiggert<nobr><em>*</em></nobr></a>,</nobr>
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~tomlin/" target="_blank">Claire J. Tomlin</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          ACM/IEEE International Conference on Human Robot Interaction (HRI), 2021
          <br>
        <nobr><em style="color:orange;">Best Paper Award Finalist</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2021HRI-abstract" role="button" aria-expanded="false" aria-controls="bobu2021HRI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/FERL/bobu2021HRI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/feature-learning" target="_blank">Project Website</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/tanmayy24/FERL" target="_blank">Code</a>
	  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=NaD-eVri8r4&ab_channel=InterACTLab" target="_blank">Talk</a>

          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/FERL/bobu2021HRI_poster.pdf" target="_blank">Poster</a>
          
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2021HRI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
        When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.  
	</div>
        </div>
      </div>
      
    </div>
  </div>
</div>
			
</li></ol>

			
			
			</ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://humanrobotinteraction.org/2020/" target="_blank">
          HRI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2020HRI" class="col p-0">
      <h5 class="title mb-0">LESS is More: Rethinking Probabilistic Models of Human Behavior</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal*</em>,</nobr>
              
                  <nobr><a href="https://www.linkedin.com/in/dexterscobee/" target="_blank">Dexter R. R. Scobee<nobr><em>*</em></nobr></a>,</nobr>
                
                  <nobr><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" target="_blank">Jaime F. Fisac</a>,</nobr>
            
                  <nobr><a href="https://people.eecs.berkeley.edu/~sastry/" target="_blank">S. Shankar Sastry</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          ACM/IEEE International Conference on Human Robot Interaction (HRI), 2020
	      <br>
       	<nobr><em style="color:orange;">Best Paper Award Winner</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2020HRI-abstract" role="button" aria-expanded="false" aria-controls="bobu2020HRI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/less/bobu2020HRI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/view/less-human-decision-model" target="_blank">Project Website</a>
	        <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=xa_l5HeyVgw" target="_blank">Talk</a>

          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/less/bobu2020HRI_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2020HRI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
			
<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="http://www.hripioneers.info/hri20/index.html" target="_blank">
          Pioneers
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2020HRIpioneers" class="col p-0">
      <h5 class="title mb-0">Detecting Hypothesis Space Misspecification in Robot Learning from Human Input</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal</em></nobr>
              
      </div>

      <div>
        <p class="periodical font-italic">
          HRI Pioneers Companion of the ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2020.
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2020HRIpioneers-abstract" role="button" aria-expanded="false" aria-controls="bobu2020HRIpioneers-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2020HRIpioneers.pdf" target="_blank">PDF</a>

          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2020HRIpioneers_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="bobu2020HRIpioneers-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Learning from human input has enabled autonomous agents to perform increasingly more complex tasks that are otherwise difficult to carry out automatically. To this end, recent works have studied how robots can incorporate such input - like demonstrations or corrections - into objective functions describing the desired behaviors. While these methods have shown progress in a variety of settings, from semi-autonomous driving, to household robotics, to automated airplane control, they all suffer from the same crucial drawback: they implicitly assume that the person's intentions can always be captured by the robot's hypothesis space. We call attention to the fact that this assumption is often unrealistic, as no model can completely account for every single possible situation ahead of time. When the robot's hypothesis space is misspecified, human input can be unhelpful - or even detrimental - to the way the robot is performing its tasks. Our work tackles this issue by proposing that the robot should first explicitly reason about how well its hypothesis space can explain human inputs, then use that situational confidence to inform how it should incorporate them.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>

			
			
			</ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2019</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.ieee-ras.org/publications/t-ro" target="_blank">
          T-RO
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2019TRO" class="col p-0">
      <h5 class="title mb-0">Quantifying Hypothesis Space Misspecification in Learning from Human-Robot Demonstrations and Physical Corrections</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal</em>,</nobr>
              
                  <nobr><a href="https://people.eecs.berkeley.edu/~abajcsy/" target="_blank">Andrea Bajcsy<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" target="_blank">Jaime F. Fisac</a>,</nobr>
            
                  <nobr><a href="https://www.researchgate.net/scientific-contributions/2133993191_Sampada_Deglurkar" target="_blank">Sampada Deglurkar</a>,</nobr>
              
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          IEEE Transactions on Robotics (T-RO)
        <br>
        <nobr><em style="color:orange;">Best Paper Award Honorable Mention</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2019TRO-abstract" role="button" aria-expanded="false" aria-controls="bobu2019TRO-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2019TRO.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/tanmayy24/jaco_learning" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2019TRO_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2019TRO-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Human input has enabled autonomous systems to improve their capabilities and achieve complex behaviors that are otherwise challenging to generate automatically. Recent work focuses on how robots can use such input - like demonstrations or corrections - to learn intended objectives. These techniques assume that the human’s desired objective already exists within the robot’s hypothesis space. In reality, this assumption is often inaccurate: there will always be situations where the person might care about aspects of the task that the robot does not know about. Without this knowledge, the robot cannot infer the correct objective. Hence, when the robot’s hypothesis space is misspecified, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. In this paper, we posit that the robot should reason explicitly about how well it can explain human inputs given its hypothesis space and use that situational confidence to inform how it should incorporate human input. We demonstrate our method on a 7 degree-of-freedom robot manipulator in learning from two important types of human input: demonstrations of manipulation tasks, and physical corrections during the robot’s task execution.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
  </div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2018</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://sites.google.com/a/robot-learning.org/corl2017/corl2018" target="_blank">
          CoRL
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2018CoRL" class="col p-0">
      <h5 class="title mb-0">Learning under Misspecified Objective Spaces</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal</em>,</nobr>
              
                  <nobr><a href="https://people.eecs.berkeley.edu/~abajcsy/" target="_blank">Andrea Bajcsy<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" target="_blank">Jaime F. Fisac</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~anca/" target="_blank">Anca D. Dragan</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          Conference on Robot Learning (CoRL), 2018
          <br>
          <nobr><em style="color:orange;">Invited to Special Issue</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2018CoRL-abstract" role="button" aria-expanded="false" aria-controls="bobu2018CoRL-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2018CoRL.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/tanmayy24/beta_adaptive_pHRI" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/stnFye8HdcU" target="_blank">Video</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/lumos/bobu2018CoRL_poster.pdf" target="_blank">Poster</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/FSsEqEJKo8A?t=6353" target="_blank">Talk</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2018CoRL-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human's desired objective lies within the robot's hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus specifically on learning from physical human corrections during the robot's task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human's correction is for the robot's hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a 7DoF robot manipulator.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://iclr.cc/Conferences/2018" target="_blank">
          ICLR
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="bobu2018ICLR" class="col p-0">
      <h5 class="title mb-0">Adapting to Continuously Shifting Domains</h5>
      <div class="author">
                
									<nobr><em>Tanmay Khandelwal</em>,</nobr>
              
                  <nobr><a href="https://scholar.google.com/citations?user=nABXo3sAAAAJ&hl=en" target="_blank">Eric Tzeng<nobr><em></em></nobr></a>,</nobr>
                
                  <nobr><a href="https://www.cc.gatech.edu/~judy/" target="_blank">Judy Hoffman</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          International Conference on Learning Representations (ICLR) Workshop, 2018
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#bobu2018ICLR-abstract" role="button" aria-expanded="false" aria-controls="bobu2018ICLR-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CUA/bobu2018ICLR.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CUA/bobu2018ICLR_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="bobu2018ICLR-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Domain adaptation typically focuses on adapting a model from a single source domain to a target domain. However, in practice, this paradigm of adapting from one source to one target is limiting, as different aspects of the real world such as illumination and weather conditions vary continuously and cannot be effectively captured by two static domains. Approaches that attempt to tackle this problem by adapting from a single source to many different target domains simultaneously are consistently unable to learn across all domain shifts. Instead, we propose an adaptation method that exploits the continuity between gradually varying domains by adapting in sequence from the source to the most similar target domain. By incrementally adapting while simultaneously efficiently regularizing against prior examples, we obtain a single strong model capable of recognition within all observed domains. Our method is applicable on a wide variety of learning settings, including visual classification and reinforcement learning in a video game domain.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


			</ol>
    </div>
  </div>




<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2016</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
			
		<li><div class="row m-0 mt-3 p-0">
			 <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;" href="https://www.miccai2016.org/en/" target="_blank">
          Patch-MI
        </a>
			</div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="dalca2016MICCAI" class="col p-0">
      <h5 class="title mb-0">Patch-Based Discrete Registration of Clinical Brain Images</h5>
      <div class="author">

                  <nobr><a href="http://www.mit.edu/~adalca/" target="_blank">Adrian Dalca<nobr><em></em></nobr></a>,</nobr>
                
									<nobr><em>Tanmay Khandelwal</em>,</nobr>
              
                  <nobr><a href="https://www.massgeneral.org/doctors/17477/natalia-rost" target="_blank">Natalia S. Rost</a>,</nobr>
            
									and
                
                  <nobr><a href="https://people.csail.mit.edu/polina/" target="_blank">Polina Golland</a>.</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
	Patch-based Techniques in Medical Imaging at the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI-PATCHMI), 2016
	<br>
	<nobr><em style="color:orange;">Best Paper Award Winner</em></nobr>
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#dalca2016MICCAI-abstract" role="button" aria-expanded="false" aria-controls="dalca2016MICCAI-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/PBR/dalca2016MICCAI.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/adalca/patchRegistration" target="_blank">Code</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/PBR/dalca2016MICCAI_poster.pdf" target="_blank">Poster</a>
        
      </div>
    
      <div class="col mt-2 p-0">
        <div id="dalca2016MICCAI-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
						We introduce a method for registration of brain images acquired in clinical settings. The algorithm relies on three-dimensional patches in a discrete registration framework to estimate correspondences. Clinical images present significant challenges for computational analysis. Fast acquisition often results in images with sparse slices, severe artifacts, and variable fields of view. Yet, large clinical datasets hold a wealth of clinically relevant information. Despite significant progress in image registration, most algorithms make strong assumptions about the continuity of image data, failing when presented with clinical images that violate these assumptions. In this paper, we demonstrate a non-rigid registration method for aligning such images. The method explicitly models the sparsely available image information to achieve robust registration. We demonstrate the algorithm on clinical images of stroke patients. The proposed method outperforms state of the art registration algorithms and avoids catastrophic failures often caused by these images. We provide a freely available open source implementation of the algorithm.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>


			</ol>
    </div>
  </div>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2024 Tanmay Khandelwal.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
